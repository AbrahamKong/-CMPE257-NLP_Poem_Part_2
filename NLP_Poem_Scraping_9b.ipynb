{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Poetry_Part_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrahamKong/-CMPE257-NLP_Poem_Part_2/blob/main/NLP_Poem_Scraping_9b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zVzMVJSaP4q"
      },
      "source": [
        " \n",
        "\n",
        "#NLP Part 1 : Poetry Analysis\n",
        "\n",
        "*   NLP Basics\n",
        "*   NER, POS, vectorization / embeddings\n",
        "*   Word2Vec, Glove, Cove, FastText, etc.\n",
        "*.  Similarity of words, context\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. NLP Data. Pick 2 poets, scrape 10 poems each . Put them in a json datastructure and save it under the poets name, in your team drive under folder 'DataSets' in the drive shared with you. \n",
        "\n",
        "2. CLAIM YOUR POETS!!! Note: your team has  to claim their 2 poets by entering in their names in this spreadsheet with a timestamp of when you entered this: no duplicate poets allowed !\n",
        "https://docs.google.com/spreadsheets/d/1-KQVrGrbz5PvLhJNXhWm55WPIpmJPvsQQ2r9U-ZieGw/edit#gid=0\n",
        "\n",
        "---for each poem of each poet:\n",
        "3. Run POS and extract verbs, nouns and adjectives (use notebook given as an example)\n",
        "\n",
        "4. Summarize each poem \n",
        "\n",
        "5. Store the summary, POS data in your json poet data structure and save to disk.\n",
        "\n",
        "6. Interchange the verbs and adjectives of the two poets based on similar words in the other poets vocabulary of verbs and adjectives\n",
        "\n",
        "6. a. save the data structure for the new poems:  poet1-poet2-poem1.json (replace poet1 adjectives and verbs with poet2's ) and poet2-poet1-poem1.json \n",
        "\n",
        "6. b. print out the new poems and save them to disk poet1-poet2.txt and poet2-poet1.txt [these files will contain 10 poems each)].\n",
        "remember to measure and compare the cohesion and perplexity of the original poems and the new poems you generate !!\n",
        "\n",
        "7. Combine poet1's and poet2's poem text and summarize it using the transformer\n",
        "\n",
        "8. Run topic modeling on the combined text of each poet: what are the top 5 topics they are writing about?\n",
        "\n",
        "9. Name the topics: use the new notebook : name_your_topic_background.ipynb (Links to an external site.) , to try and find the closest word that describes topic 0, topic 1, topic 2 : instead of topic 0, what is the closest word in that cluster that describes the topic?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Claim 2 poets and Scrape 10 poems each\n",
        "John Keats\n",
        "Alfred Tennyson    \n",
        "\n",
        "Scraping example colab: https://colab.research.google.com/drive/1Px_UggyRiQJzrIxvLtitz-Es0aZ5Au8b?authuser=3#scrollTo=3aTGAo7rMvjN&uniqifier=1\n",
        "\n"
      ],
      "metadata": {
        "id": "GRKtsLxDZAxR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWzkVGilbE7A"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install gensim\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "!pip install pyLDAvis\n",
        "!pip install spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYwGuPWGbMT4"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjSMlDx2bOpP"
      },
      "source": [
        "!pip install -U pandas-profiling"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vpDCDPrbRE4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "e5d00928-bb26-497e-f28b-0cf34d63a3a7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import wordcloud\n",
        "import nltk\n",
        "import unicodedata\n",
        "import contractions\n",
        "import pickle\n",
        "import urllib.request\n",
        "import requests\n",
        "import spacy\n",
        "import json\n",
        "%matplotlib inline\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from collections import defaultdict\n",
        "from contractions import contractions_dict\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from transformers import pipeline\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from spacy import displacy\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "from contractions import contractions_dict\n",
        "from scipy import stats\n",
        "from scipy import sparse\n",
        "\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "scaler = MinMaxScaler()\n",
        "std_scaler = StandardScaler()\n",
        "stops = set(stopword_list)\n",
        "stemmer = nltk.stem.SnowballStemmer('english')\n",
        "wordlemmatizer = WordNetLemmatizer()\n",
        "tokenizer = nltk.word_tokenize"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-15d64c11ca77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contractions'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Data. Pick 2 poets, scrape 10 poems each . Put them in a json datastructure and save it under the poets name, in your team drive under folder 'DataSets'"
      ],
      "metadata": {
        "id": "6D_Gk07BYyVK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egcqOhvLbgN-"
      },
      "source": [
        "## Get Scraped Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NSzUHrObssV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72SL8SR1ca2f"
      },
      "source": [
        "# df_john = pd.read_csv('/content/drive/MyDrive/DataSets/john_keats.csv', sep=';')\n",
        "lh_df = pd.read_csv('https://raw.githubusercontent.com/AbrahamKong/-CMPE257-NLP_Poem_Part_2/main/langston_hughes.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5uLsDvzcdBu"
      },
      "source": [
        "lh_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WNQiFZLce-m"
      },
      "source": [
        "# df_alfred = pd.read_csv('/content/drive/MyDrive/DataSets/alfred_tennyson.csv', sep=';')\n",
        "mjo_df = pd.read_csv('https://raw.githubusercontent.com/AbrahamKong/-CMPE257-NLP_Poem_Part_2/main/mary_j_oliver.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_AD-ieOcg-2"
      },
      "source": [
        "mjo_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFbXiyq8ck5A"
      },
      "source": [
        "## JSON Conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ll64T8GSZelt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hs9sU4eckn_"
      },
      "source": [
        "result_lh = lh_df.to_json(orient=\"index\")\n",
        "parsed_lh = json.loads(result_lh)\n",
        "json.dumps(parsed_lh, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnJ1pDYPc06n"
      },
      "source": [
        "# with open('/content/drive/MyDrive/DataSets/john_keats_final.txt', 'w') as outfile:\n",
        "#     json.dump(result_john, outfile)\n",
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/langston_hughes.csv', 'w') as outfile:\n",
        "    json.dump(result_lh, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6ArqW4Rc3dA"
      },
      "source": [
        "result_mjo = mjo_df.to_json(orient=\"index\")\n",
        "parsed_mjo = json.loads(result_mjo)\n",
        "json.dumps(parsed_mjo, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n81I-1-3c5rI"
      },
      "source": [
        "# with open('/content/drive/MyDrive/DataSets/alfred_tennyson_final.txt', 'w') as outfile:\n",
        "#     json.dump(result_alfred, outfile)\n",
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/mary_j_oliver.csv', 'w') as outfile:\n",
        "    json.dump(result_mjo, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZGBn_URc7w3"
      },
      "source": [
        "parsed_mjo[\"0\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRLnisPgc-xg"
      },
      "source": [
        "## Build JSON Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcgPLkxodC35"
      },
      "source": [
        "langston_hughes = {\n",
        "\n",
        "}\n",
        "\n",
        "mary_j_oliver = {\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE51185-dHhg"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVm7HINDdK-v"
      },
      "source": [
        "def cleanPoemText(text):\n",
        "    \n",
        "\n",
        "    text = contractions.fix(text)\n",
        "    \n",
        "    text = text.strip().lower()\n",
        "    \n",
        "    text = re.sub(r\"'\", ' ', text)\n",
        "    \n",
        "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    translate_dict = dict((i, \" \") for i in filters)\n",
        "    translate_map = str.maketrans(translate_dict)\n",
        "    text = text.translate(translate_map)\n",
        "\n",
        "    text = ' '.join([w for w in text.split()])\n",
        "\n",
        "    # Replace multiple space with one space\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    \n",
        "    text = ''.join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIxqkmAfdUy_"
      },
      "source": [
        "## Clean Poem Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxWyRjwRdSIw"
      },
      "source": [
        "embedding = ''\n",
        "def buildPoemArray(poet, data):\n",
        "  poems = []\n",
        "  for i in data:\n",
        "    poem = data[i][\"Poem\"]\n",
        "\n",
        "    clean_poem = cleanPoemText(poem)\n",
        "    idx = \"poem\" + str(i)\n",
        "    poet[idx] = clean_poem\n",
        "    poems.append(idx)\n",
        "  poet['poems_array'] = poems\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol2fgkkida3Y"
      },
      "source": [
        "buildPoemArray(langston_hughes, parsed_lh)\n",
        "buildPoemArray(mary_j_oliver, parsed_mjo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW-WVzNedkug"
      },
      "source": [
        "langston_hughes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1YnnpDJdjHw"
      },
      "source": [
        "mary_j_oliver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS2XXjIXXhnA"
      },
      "source": [
        "langston_hughes_poems = langston_hughes.copy()\n",
        "mary_j_oliver_poems = mary_j_oliver.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STzqcZPUdnWp"
      },
      "source": [
        "# Run Part-Of-Speech, extract verbs, nouns and adjectives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ty5NNaXdsQI"
      },
      "source": [
        "def getPOS(poet):\n",
        "  global_vrb= set()\n",
        "  global_nns= set()\n",
        "  global_adj= set()\n",
        "  for key in poet['poems_array']:\n",
        "    text = poet[key]\n",
        "    wrd_list = word_tokenize(text)\n",
        "    pos_val = nltk.pos_tag(wrd_list)\n",
        "    poet[\"pos_\"+key] = pos_val\n",
        "\n",
        "    vrb = set([word for (word, pos) in pos_val if (pos.startswith('VB'))])\n",
        "    nns = set([word for (word, pos) in pos_val if (pos.startswith('NN'))])\n",
        "    adj = set([word for (word, pos) in pos_val if (pos.startswith('JJ'))])\n",
        "\n",
        "    poet[\"verbs_\"+key] = list(vrb)\n",
        "    poet[\"adjectives_\"+key] = list(adj)\n",
        "    poet[\"nouns_\"+key] = list(nns)\n",
        "\n",
        "    global_vrb = set.union(global_vrb, vrb)\n",
        "    global_nns = set.union(global_nns, nns)\n",
        "    global_adj = set.union(global_adj, adj)\n",
        "\n",
        "  poet['all_verbs'] = list(global_vrb)\n",
        "  poet['all_nouns'] = list(global_nns)\n",
        "  poet['all_adjectives'] = list(global_adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_zt9homdxTh"
      },
      "source": [
        "langston_hughes_pos = langston_hughes.copy()\n",
        "getPOS(langston_hughes_pos)\n",
        "langston_hughes_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNC9ffYSKLrZ"
      },
      "source": [
        "langston_hughes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukh7Wwo-duvb"
      },
      "source": [
        "mary_j_oliver_pos = mary_j_oliver.copy()\n",
        "getPOS(mary_j_oliver_pos)\n",
        "mary_j_oliver_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0WP1jVVJ2Gj"
      },
      "source": [
        "mary_j_oliver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTru-dPsWFv4"
      },
      "source": [
        "# Store the POS JSON Data to File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWURKKYVWDoJ"
      },
      "source": [
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/langston_hughes_pos_final.json', 'w') as outfile:\n",
        "    json.dump(langston_hughes_pos, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFu2hF4_WE8n"
      },
      "source": [
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/mary_j_oliver_pos_final.json', 'w') as outfile:\n",
        "    json.dump(mary_j_oliver_pos, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkEcC9_Nd3Sx"
      },
      "source": [
        "# Summarization using transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBwaKRCEd-wi"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMQNMhUSeA25"
      },
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmAgCLqReDf5"
      },
      "source": [
        "def getSummaryOfPoem(poet):\n",
        "  def removeQuotes(mydata):\n",
        "    new_data = mydata.strip('“”')\n",
        "    return new_data\n",
        "  count = 0\n",
        "  for key in poet['poems_array']:\n",
        "    count += 1;\n",
        "    text = poet[key] \n",
        "    text = removeQuotes(text)\n",
        "    summary_text = summarizer(text, max_length=512, min_length=5, do_sample=False)[0]['summary_text']\n",
        "    poet[\"summary_\"+key] = summary_text\n",
        "  \n",
        "  return poet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import gmtime, strftime\n",
        "print (strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
        "\n",
        "from time import gmtime, strftime\n",
        "print (strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
        "\n",
        "import timeit\n",
        "\n",
        "start = timeit.default_timer()"
      ],
      "metadata": {
        "id": "Tfm4B2obUiAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI2d-0LJeF5h"
      },
      "source": [
        "langston_hughes_sum = langston_hughes.copy()\n",
        "mary_j_oliver_sum = mary_j_oliver.copy()\n",
        "data_langston_hughes = getSummaryOfPoem(langston_hughes_sum)\n",
        "data_mary_j_oliver = getSummaryOfPoem(mary_j_oliver_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Your statements here\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "\n",
        "print('Time: ', stop - start)  "
      ],
      "metadata": {
        "id": "SL752lNnUkwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuGabMI3eKp5"
      },
      "source": [
        "langston_hughes_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-1c0LCGeIkJ"
      },
      "source": [
        "mary_j_oliver_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYotiHzAvrag"
      },
      "source": [
        "data_mary_j_oliver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie0nLFYvePBi"
      },
      "source": [
        "# Store the summarized JSON Data to File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyoZF3SUeN3E"
      },
      "source": [
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/langston_hughes_summary_final.json', 'w') as outfile:\n",
        "    json.dump(langston_hughes_sum, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIpHsLZyeWHW"
      },
      "source": [
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/mary_j_oliver_summary_final.json', 'w') as outfile:\n",
        "    json.dump(mary_j_oliver_sum, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXKu75VheZPh"
      },
      "source": [
        "# Interchange the Verbs and Adjectives for both the poets using transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIUiz_t_egzK"
      },
      "source": [
        "model = SentenceTransformer('stsb-roberta-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0PXvCGzeizB"
      },
      "source": [
        "def getSemanticSimilarity(p1,p2):\n",
        "  embedding1 = model.encode(p1, convert_to_tensor=True)\n",
        "  embedding2 = model.encode(p2, convert_to_tensor=True)\n",
        "  cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "  print(cosine_scores) \n",
        "  similarity_score = cosine_scores.item()\n",
        "  return similarity_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoVP7zT3ekoi"
      },
      "source": [
        "getSemanticSimilarity(langston_hughes_pos['all_verbs'][0],mary_j_oliver_pos[\"all_verbs\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HhOOh45e7w0"
      },
      "source": [
        "# Swapping the Verbs based on similarity score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO1ZylO3eo0a"
      },
      "source": [
        "\n",
        "def swapVerbs(p1,p2,text):\n",
        "    for v1 in p1:\n",
        "      for v2 in p2:\n",
        "        score = getSemanticSimilarity(v1,v2)\n",
        "        if score > 0.3:\n",
        "          text = text.replace(v1,v2)\n",
        "          print(\"verb\")\n",
        "          print(text)\n",
        "          break\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9TYbcUIe4fj"
      },
      "source": [
        "# Swapping the Adjectives based on similarity score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtazFwpUi2mD"
      },
      "source": [
        "def swapAdjectives(p1,p2, poem):\n",
        "  text = poem\n",
        "  for a1 in p1:\n",
        "    for a2 in p2:\n",
        "      print(\"inside loop 2\")\n",
        "      print(a1)\n",
        "      print(a2)\n",
        "      print(text)\n",
        "      score = getSemanticSimilarity(a1,a2)\n",
        "      if score > 0.3:\n",
        "        text = text.replace(a1,a2)\n",
        "        print(\"adjective\")\n",
        "        print(text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9tNA0fUaPaP"
      },
      "source": [
        "def swapIteration(poet1, poet2, text):\n",
        "    p2_verbs = poet2[\"all_verbs\"]\n",
        "    p1_verbs = poet1[\"all_verbs\"]\n",
        "    p2_verbs = p2_verbs[0:2]\n",
        "    p1_verbs = p1_verbs[0:2]\n",
        "    print(p1_verbs)\n",
        "    print(p2_verbs)\n",
        "    new_text = swapVerbs(p1_verbs, p2_verbs, text)\n",
        "    print(\"new text verbs\")\n",
        "    print(new_text)\n",
        "    # print(text)\n",
        "    p2_adjectives = poet2[\"all_adjectives\"]\n",
        "    p1_adjectives = poet1[\"all_adjectives\"]\n",
        "    p2_adjectives = p2_adjectives[0:2]\n",
        "    p1_adjectives = p1_adjectives[0:2]\n",
        "    brand_new_text = swapAdjectives(p1_adjectives, p2_adjectives, new_text)\n",
        "    print(\"new text adjectives\")\n",
        "    print(brand_new_text)\n",
        "    return brand_new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo9rTPvOcgpo"
      },
      "source": [
        "John Poems Swapped Verbs and Adjectives of Alfred Tennyson Poems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHhrUpvpb8PU"
      },
      "source": [
        "langston_hughes_poems_2 = langston_hughes_poems.copy()\n",
        "langston_hughes_poems_2.pop('poems_array')\n",
        "langston_hughes_poems_swapped=[]\n",
        "for i in langston_hughes_poems:\n",
        "  print(\"poems:\")\n",
        "  print(i)\n",
        "  # inx = 'poem'+str(i);\n",
        "  poem_str = str(langston_hughes_poems[i]);\n",
        "  print(poem_str)\n",
        "\n",
        "  langston_hughes_poems_swapped.append(swapIteration(langston_hughes_pos, mary_j_oliver_pos, poem_str))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printing the new poems after swapping"
      ],
      "metadata": {
        "id": "Rn6xR_U4a3Vz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93K-jvkKTMT1"
      },
      "source": [
        "langston_hughes_poems_swapped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: We need to format the poem in a human readable manner"
      ],
      "metadata": {
        "id": "6Tl4MTeJYlAx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm27KlV3c0WY"
      },
      "source": [
        " Alfred Tennyson Poems Swapped Verbs  and Adjectives of John Poems\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VNs3KQ4TzRh"
      },
      "source": [
        "mary_j_oliver_poems_swapped = []\n",
        "mary_j_oliver_poems_2 = mary_j_oliver_poems.copy()\n",
        "mary_j_oliver_poems_2.pop('poems_array')\n",
        "for i in mary_j_oliver_poems:\n",
        "  print(\"poems:\")\n",
        "  print(i)\n",
        "  # inx = 'poem'+str(i);\n",
        "  poem_str_2 = str(mary_j_oliver_poems[i]);\n",
        "  print(poem_str_2)\n",
        "\n",
        "  mary_j_oliver_poems_swapped.append(swapIteration(langston_hughes_pos, mary_j_oliver_pos, poem_str_2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printing the new poems after swapping"
      ],
      "metadata": {
        "id": "9SM1c3FqbAm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mary_j_oliver_poems_swapped"
      ],
      "metadata": {
        "id": "u16Vbh9Gat28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NY5r9IF1iCi"
      },
      "source": [
        "# Swapped Poem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa_Ew-EueUDY"
      },
      "source": [
        "def buildSwappedPoemArray(poet,data):\n",
        "  poems = []\n",
        "  data_length = len(data)\n",
        "  for i in range(data_length):\n",
        "    poem = data[i]\n",
        "    #clean_poem = cleanPoemText(poem)\n",
        "    idx = \"poem\" + str(i)\n",
        "    poet[idx] = poem\n",
        "    poems.append(idx)\n",
        "  poet['poems_swapped_array'] = poems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pJJOMfSfjQw"
      },
      "source": [
        "langston_hughes_swapped = {\n",
        "\n",
        "}\n",
        "\n",
        "mary_j_oliver_swapped = {\n",
        "\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqhhHKnd0LSr"
      },
      "source": [
        "langston_hughes_poems_swapped\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z9D9nsoez8A"
      },
      "source": [
        "buildSwappedPoemArray(langston_hughes_swapped,langston_hughes_poems_swapped)\n",
        "buildSwappedPoemArray(mary_j_oliver_swapped,mary_j_oliver_poems_swapped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq58_xnS3oFg"
      },
      "source": [
        "langston_hughes_swapped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDqZzu0I19uR"
      },
      "source": [
        "langston_hughes_swapped_to_json_file = langston_hughes_swapped.copy()\n",
        "langston_hughes_swapped_to_json_file.pop('poems_swapped_array')\n",
        "langston_hughes_swapped_to_json_file.pop('poem10')\n",
        "\n",
        "mary_j_oliver_swapped_to_json_file = mary_j_oliver_swapped.copy()\n",
        "mary_j_oliver_swapped_to_json_file.pop('poems_swapped_array')\n",
        "mary_j_oliver_swapped_to_json_file.pop('poem10')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN20R3J53To2"
      },
      "source": [
        "langston_hughes_swapped_to_json_file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6nrTRJ53bmm"
      },
      "source": [
        "mary_j_oliver_swapped_to_json_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsPOLhgCgKyx"
      },
      "source": [
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/langston_hughes_swapped_to_json_file_final.json', 'w') as outfile:\n",
        "    json.dump(langston_hughes_swapped_to_json_file, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weZrG3kUgMT4"
      },
      "source": [
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/mary_j_oliver_swapped_to_json_file_final.json', 'w') as outfile:\n",
        "    json.dump(mary_j_oliver_swapped_to_json_file, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CMquzz149ft"
      },
      "source": [
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/langston_hughes_swapped_to_txt_file_final.txt', 'w') as outfile:\n",
        "    json.dump(langston_hughes_swapped_to_json_file, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOJqbiCt4_1c"
      },
      "source": [
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/mary_j_oliver_swapped_to_txt_file_final.txt', 'w') as outfile:\n",
        "    json.dump(mary_j_oliver_swapped_to_json_file, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7iSM93070Fq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import wordcloud\n",
        "import nltk\n",
        "import unicodedata\n",
        "import contractions\n",
        "import pickle\n",
        "import urllib.request\n",
        "import requests\n",
        "import spacy\n",
        "import json\n",
        "%matplotlib inline\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from collections import defaultdict\n",
        "from contractions import contractions_dict\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from transformers import pipeline\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from spacy import displacy\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "from contractions import contractions_dict\n",
        "from scipy import stats\n",
        "from scipy import sparse\n",
        "\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "scaler = MinMaxScaler()\n",
        "std_scaler = StandardScaler()\n",
        "stops = set(stopword_list)\n",
        "stemmer = nltk.stem.SnowballStemmer('english')\n",
        "wordlemmatizer = WordNetLemmatizer()\n",
        "tokenizer = nltk.word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGEq0zlZ7IQG"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLNMBbrC9LCG"
      },
      "source": [
        "langston_hughes_choherence_input = langston_hughes.copy()\n",
        "langston_hughes_choherence_input.pop('poems_array')\n",
        "langston_hughes_choherence_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30zPEt_sXO9a"
      },
      "source": [
        "swapped_langston_hughes_choherence_input = langston_hughes_swapped.copy()\n",
        "swapped_langston_hughes_choherence_input.pop('poems_swapped_array')\n",
        "swapped_langston_hughes_choherence_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHbkQ1Qd9OW-"
      },
      "source": [
        "coherence_perplexity_calculator = mary_j_oliver.copy()\n",
        "coherence_perplexity_calculator.pop('poems_array')\n",
        "coherence_perplexity_calculator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF0b2-5qXZPZ"
      },
      "source": [
        "swapped_coherence_perplexity_calculator = mary_j_oliver_swapped.copy()\n",
        "swapped_coherence_perplexity_calculator.pop('poems_swapped_array')\n",
        "swapped_coherence_perplexity_calculator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -upgrade gensim"
      ],
      "metadata": {
        "id": "Ruah_uokk8Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "AuOSJDAVs_ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Topic Modelling on combined text"
      ],
      "metadata": {
        "id": "elmrguk-QUf2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tirZLm5zSWD6"
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "def choherence_perplexity_calculator(poem_list):\n",
        "  def sent_to_words(sentences):\n",
        "      for sentence in sentences:\n",
        "          yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "  data_words = list(sent_to_words(poem_list))\n",
        "\n",
        "  print(data_words[:1])\n",
        "  # Build the bigram and trigram models\n",
        "  bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "  trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "  # Faster way to get a sentence clubbed as a trigram/bigram\n",
        "  bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "  trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "  # See trigram example\n",
        "  print(trigram_mod[bigram_mod[data_words[0]]])\n",
        "  # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "  def remove_stopwords(texts):\n",
        "      return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "  def make_bigrams(texts):\n",
        "      return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "  def make_trigrams(texts):\n",
        "      return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "  def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "      \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "      texts_out = []\n",
        "      for sent in texts:\n",
        "          doc = nlp(\" \".join(sent)) \n",
        "          texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "      return texts_out\n",
        "  # Remove Stop Words\n",
        "  #data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "  # Form Bigrams\n",
        "  data_words_bigrams = make_bigrams(data_words)\n",
        "\n",
        "  # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "  # python3 -m spacy download en\n",
        "  nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "  # Do lemmatization keeping only noun, adj, vb, adv\n",
        "  data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN']) #, 'ADJ', 'VERB', 'ADV'\n",
        "\n",
        "  print(data_lemmatized[:1])\n",
        "  # Create Dictionary\n",
        "  id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "  # Create Corpus\n",
        "  texts = data_lemmatized\n",
        "\n",
        "  # Term Document Frequency\n",
        "  corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "  # View\n",
        "  print(corpus[:1])\n",
        " \n",
        "  # lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "  #                                           id2word=id2word,\n",
        "  #                                           num_topics=10, \n",
        "  #                                           random_state=100,\n",
        "  #                                           update_every=1,\n",
        "  #                                           chunksize=100,\n",
        "  #                                           passes=10,\n",
        "  #                                           alpha='auto',\n",
        "  #                                           per_word_topics=True)\n",
        "  lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=10, \n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)\n",
        "  print(lda_model.print_topics())\n",
        "  # Visualize the topics\n",
        "  pyLDAvis.enable_notebook()\n",
        "  vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "  # vis\n",
        "  # Compute Perplexity\n",
        "  print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "  # Compute Coherence Score\n",
        "  coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "  coherence_lda = coherence_model_lda.get_coherence()\n",
        "  print('\\nCoherence Score: ', coherence_lda)\n",
        "  return vis\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB-o8LE2TsxS"
      },
      "source": [
        "poem_count = len(langston_hughes_choherence_input)\n",
        "langston_poem_list=[]\n",
        "for j in langston_hughes_choherence_input:\n",
        "  poem_index =  str(j)\n",
        "  print(poem_index)\n",
        "  print(langston_hughes_choherence_input[poem_index])\n",
        "  langston_poem_list.append(langston_hughes_choherence_input[poem_index])\n",
        "print(langston_poem_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBqLIyh9R0Xi"
      },
      "source": [
        "# Original Langston Hughes Poem's perplexity and coherence score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install attrs==19.1.0"
      ],
      "metadata": {
        "id": "1pP5nvKM0Zyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas --upgrade\n",
        "# !pip install pandas==1.3.1"
      ],
      "metadata": {
        "id": "vFSru7mDwRuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmAJi77dVSED"
      },
      "source": [
        "vis = choherence_perplexity_calculator(langston_poem_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis"
      ],
      "metadata": {
        "id": "m4YuY16EUo0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpmyyIJT24Tv"
      },
      "source": [
        "# **Love** is the Topic Name from based on the LDA model topics from Langston Hughes Poem's  (child, year, dream, night, sun)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtK9FSesTqZS"
      },
      "source": [
        "poem_count = len(coherence_perplexity_calculator)\n",
        "mary_poem_list=[]\n",
        "for j in coherence_perplexity_calculator:\n",
        "  poem_index =  str(j)\n",
        "  print(poem_index)\n",
        "  print(coherence_perplexity_calculator[poem_index])\n",
        "  mary_poem_list.append(coherence_perplexity_calculator[poem_index])\n",
        "print(mary_poem_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liHlRcXwRrRJ"
      },
      "source": [
        "# Original Mary J Oliver Poem's perplexity and coherence score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMJBhaJNT7SC"
      },
      "source": [
        "vis = choherence_perplexity_calculator(mary_poem_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis"
      ],
      "metadata": {
        "id": "1wUt05RsVgF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CetsflNw3q5F"
      },
      "source": [
        "# **Love** is the Topic Name from based on the LDA model topics from Mary J Oliver Poem's  (dream, child, light, sum, year)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBKegZt3WeJz"
      },
      "source": [
        "swapped_langston_poem_list=[]\n",
        "for j in swapped_langston_hughes_choherence_input:\n",
        "  swapped_poem_index =  str(j)\n",
        "  print(swapped_poem_index)\n",
        "  print(swapped_langston_hughes_choherence_input[swapped_poem_index])\n",
        "  swapped_langston_poem_list.append(swapped_langston_hughes_choherence_input[swapped_poem_index])\n",
        "print(swapped_langston_poem_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz_PZ0ODV6Ez"
      },
      "source": [
        "# Swapped Langston Hughes Poem's perplexity and coherence score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Isbw9Z4W3dC"
      },
      "source": [
        "vis = choherence_perplexity_calculator(swapped_langston_poem_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf1nECd2WkLR"
      },
      "source": [
        "swapped_mary_poem_list=[]\n",
        "for j in swapped_coherence_perplexity_calculator:\n",
        "  swapped_poem_index =  str(j)\n",
        "  print(swapped_poem_index)\n",
        "  print(swapped_coherence_perplexity_calculator[swapped_poem_index])\n",
        "  swapped_mary_poem_list.append(swapped_coherence_perplexity_calculator[swapped_poem_index])\n",
        "print(swapped_mary_poem_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE7PbjqUWS8S"
      },
      "source": [
        "# Swapped Mary J Oliver Poem's perplexity and coherence score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB1rSERgbfXU"
      },
      "source": [
        "vis = choherence_perplexity_calculator(swapped_mary_poem_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmF0td2puaIV"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "import spacy\n",
        "\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "import joblib\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "from ipywidgets import interact, Layout, HBox, VBox, Box\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tqdm import tqdm\n",
        "from os.path import isfile\n",
        "\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"dark_background\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYfINliFvXGW"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dfP-eaYvHNs"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 3000000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r42-_r6u7T8"
      },
      "source": [
        "def spacy_tokenizer(sentence):\n",
        "    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbQZN4q7uoc8"
      },
      "source": [
        "vectorizer_langston = CountVectorizer(tokenizer = spacy_tokenizer, min_df=2)\n",
        "vectorizer_mary = CountVectorizer(tokenizer = spacy_tokenizer, min_df=2)\n",
        "\n",
        "data_vectorized_langston = vectorizer_langston.fit_transform(tqdm(langston_poem_list))\n",
        "data_vectorized_mary = vectorizer_mary.fit_transform(tqdm(mary_poem_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine poems, Summarize using tansformer and Save in file"
      ],
      "metadata": {
        "id": "MHMCEc_zA6l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combinePoems(poems, poet):\n",
        "    poems_ref = []\n",
        "    all_poems = []\n",
        "    c = 0\n",
        "    for i in poems:\n",
        "      c += 1\n",
        "      # print(i)\n",
        "      all_poems.append(i)\n",
        "      if c == 10:\n",
        "        break\n",
        "    p = {'all_poems':all_poems}\n",
        "    poems_ref.append(p)\n",
        "    po = {'poet': poet}\n",
        "    poems_ref.append(po)\n",
        "    return poems_ref"
      ],
      "metadata": {
        "id": "4R0TIxv_A5u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langston_combined = swapped_langston_poem_list.copy()\n",
        "langston_combined = combinePoems(langston_combined, 'Langston Hughes')\n",
        "langston_combined"
      ],
      "metadata": {
        "id": "gEzkb-EwECsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mary_combined = swapped_mary_poem_list.copy()\n",
        "mary_combined = combinePoems(mary_combined, 'Mary J Oliver')\n",
        "mary_combined"
      ],
      "metadata": {
        "id": "wkaqfKlxBGIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/langston_hughes_swapped_combined_final.json', 'w') as outfile:\n",
        "  json.dump(langston_combined, outfile)\n",
        "with open('/content/drive/MyDrive/SJSU/CMPE 257: Machine Learning/NLP/Homework 9b/Datasets/mary_j_oliver_swapped_combined_final.json', 'w') as outfile:\n",
        "  json.dump(mary_combined, outfile)"
      ],
      "metadata": {
        "id": "j_638TCkEPMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yXtWeSJ5sGX"
      },
      "source": [
        "# Frequent words used in Langston Hughes poems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdu8Js2kuzbH"
      },
      "source": [
        "word_count_langston = pd.DataFrame({'word': vectorizer_langston.get_feature_names(), 'count': np.asarray(data_vectorized_langston.sum(axis=0))[0]})\n",
        "\n",
        "word_count_langston.sort_values('count', ascending=False).set_index('word')[:10].sort_values('count', ascending=True).plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrnpuif_5mL9"
      },
      "source": [
        "# Frequent words used in Mary J Oliver poems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgAWJiW80mel"
      },
      "source": [
        "word_count_mary = pd.DataFrame({'word': vectorizer_mary.get_feature_names(), 'count': np.asarray(data_vectorized_mary.sum(axis=0))[0]})\n",
        "\n",
        "word_count_mary.sort_values('count', ascending=False).set_index('word')[:10].sort_values('count', ascending=True).plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Name the Topics"
      ],
      "metadata": {
        "id": "-pDnFtFBRuo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topicModeling(text1):\n",
        "  text1_list_orig = []\n",
        "  bigram_text1 = []\n",
        "  corpus_text1 = []\n",
        "\n",
        "  text1_list_orig = [gensim.utils.simple_preprocess(text1)]\n",
        "  bigram_text1 = gensim.models.Phrases(text1_list_orig) # higher threshold fewer phrases.\n",
        "  trigram_text1 = gensim.models.Phrases(bigram_text1[text1_list_orig])\n",
        "  #remove stop words\n",
        "  remove_stop_text1 = remove_stopwords(text1_list_orig)\n",
        "  # Form Bigrams\n",
        "  data_words_bigrams_text1 = make_bigrams(remove_stop_text1, bigram_text1)\n",
        "  # Do lemmatization keeping only noun, adj, vb, adv\n",
        "  data_lemmatized_text1 = lemmatization(data_words_bigrams_text1, allowed_postags=['NOUN']) # , 'ADJ', 'VERB', 'ADV'\n",
        "  id2word_text1 = corpora.Dictionary(data_lemmatized_text1)\n",
        "\n",
        "  # Term Document Frequency\n",
        "  corpus_text1 = [id2word_text1.doc2bow(text) for text in data_lemmatized_text1]\n",
        "  lda_model_text1 = gensim.models.LdaMulticore(corpus=corpus_text1,\n",
        "                                       id2word=id2word_text1,\n",
        "                                       num_topics=10, \n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)\n",
        "  doc_lda_text1 = lda_model_text1[corpus_text1]\n",
        "  return lda_model_text1, corpus_text1, data_lemmatized_text1, id2word_text1\n",
        "\n"
      ],
      "metadata": {
        "id": "njTfbJZARzl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNkA2PT44Lcg"
      },
      "source": [
        "# **Conclusion**\n",
        "Based on the frequency count of words, below are the top 3 topic names for both the poets:   \n",
        "**Langston Hughes:** \n",
        "1. dream\n",
        "2. educational\n",
        "3. dark   \n",
        "**Mary J Oliver:**\n",
        "1. owner\n",
        "2. provide\n",
        "3. purpose\n",
        "\n",
        "Based on the LDA weights, below are the top 3 topic names for both the poets:      \n",
        "**Langston Hughes:** \n",
        "1. little \n",
        "2. purpose \n",
        "3. letter  \n",
        "\n",
        "**Mary J Oliver:**\n",
        "1. dream\n",
        "2. child\n",
        "3. light"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "1. https://github.com/aarsanjani/nlp-poets\n",
        "2. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "3. https://sjsu.instructure.com/courses/1465859/assignments/5992374\n"
      ],
      "metadata": {
        "id": "1Ud6pB9Y9Vuy"
      }
    }
  ]
}